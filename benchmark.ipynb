{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!** This notebook has to be run after the following notebooks:\n",
    "\n",
    "-   [dataset.ipynb](dataset.ipynb)\n",
    "-   [scoring.ipynb](scoring.ipynb)\n",
    "-   [model.ipynb](model.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import dataset\n",
    "import scoring\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using constant padding\n",
      "| using scales: [0.8333333333333334, 1.0, 1.2]\n",
      "| using ordinary correlation\n",
      "load pretrained model from models/SiamSE/checkpoint_vot.pth\n",
      "remove prefix \"module.\"\n",
      "missing keys:set()\n",
      "unused checkpoint keys:set()\n",
      "| using constant padding\n",
      "| using scales: [0.8333333333333334, 1.0, 1.2]\n",
      "| using ordinary correlation\n",
      "load pretrained model from models/SiamSE/checkpoint_vot.pth\n",
      "remove prefix \"module.\"\n",
      "missing keys:set()\n",
      "unused checkpoint keys:set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leiyks/git/DNN/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/leiyks/git/DNN/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "models: Dict[str, model.Tracker] = model.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/David gt and image files have different length\n",
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/Football1 gt and image files have different length\n",
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/CarScale gt and image files have different length\n",
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/Jogging2 gt and image files have different length\n",
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/Jogging1 gt and image files have different length\n",
      "Error while loading dataset /home/leiyks/git/DNN/data/mytc128/Subway gt and image files have different length\n"
     ]
    }
   ],
   "source": [
    "datasets: Dict = dataset.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['SEsiamFC', 'AAA', 'PyECO']), dict_keys(['mytc128', 'myvot2021']))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys(), datasets.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(out, image_file, gt, pred=None):\n",
    "    \"\"\"Write a frame to the video.\n",
    "\n",
    "    Args:\n",
    "        out (cv2.VideoWriter): Video writer.\n",
    "        image_file (str): Path to the image file.\n",
    "        gt (List[int]): Ground truth bounding box.\n",
    "        pred (List[int], optional): Predicted bounding box. Defaults to None.\n",
    "    \"\"\"\n",
    "    im = cv2.imread(image_file)\n",
    "    cv2.rectangle(\n",
    "        im,\n",
    "        (int(gt[0]), int(gt[1])),\n",
    "        (int(gt[0] + gt[2]), int(gt[1] + gt[3])),\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )\n",
    "    if pred is not None:\n",
    "        cv2.rectangle(\n",
    "            im,\n",
    "            (int(pred[0]), int(pred[1])),\n",
    "            (int(pred[0] + pred[2]), int(pred[1] + pred[3])),\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "        )\n",
    "    out.write(im)\n",
    "\n",
    "\n",
    "def track_video(tracker, video, verbose=0, save_video=None):\n",
    "    \"\"\"Track a whole video.\n",
    "\n",
    "    Args:\n",
    "        tracker (model.Tracker): Tracker.\n",
    "        video (Dict): Video.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "        save_video (str, optional): Path to the video to save. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Precision, Success, FPS.\n",
    "    \"\"\"\n",
    "    start_frame, toc = 0, 0\n",
    "\n",
    "    pred = []\n",
    "    image_files, gt = video[\"image_files\"], video[\"gt\"]\n",
    "\n",
    "    if save_video is not None:\n",
    "        sh = cv2.imread(image_files[0]).shape[:2]\n",
    "        frame_size = (sh[1], sh[0])\n",
    "        out = cv2.VideoWriter(\n",
    "            save_video, cv2.VideoWriter_fourcc(*\"DIVX\"), 20.0, frame_size\n",
    "        )\n",
    "\n",
    "    for f, image_file in enumerate(image_files):\n",
    "        tic = cv2.getTickCount()\n",
    "\n",
    "        if f == start_frame:  # init\n",
    "            tracker.initialize(image_file, np.array(gt[f]))\n",
    "            pred.append(gt[f])\n",
    "\n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f])\n",
    "\n",
    "        elif f > start_frame:  # tracking\n",
    "            pred_bbox = tracker.track(image_file)\n",
    "            b_overlap = scoring.get_precision(gt[f], pred_bbox)\n",
    "            if b_overlap > 0:\n",
    "                pred.append(pred_bbox)\n",
    "            else:\n",
    "                pred.append(2)\n",
    "                start_frame = f + 5\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(f\"{f} gt: {gt[f]} pred: {pred_bbox} overlap: {b_overlap}\")\n",
    "\n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f], pred_bbox)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f])\n",
    "\n",
    "        toc += cv2.getTickCount() - tic\n",
    "\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    precisions = [scoring.get_precision(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    precisions = np.array(precisions)\n",
    "    mprec = np.mean(precisions)\n",
    "\n",
    "    success = [scoring.is_success(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    success = np.array(success)\n",
    "    msucc = np.mean(success)\n",
    "\n",
    "    fps = f / toc\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\n",
    "            f'Video: {video[\"name\"]:12s} Time: {toc:2.1f}s Speed: {fps:5.2f}fps mSuccess: {msucc:3.2f} mPrecision {mprec:3.2f}'\n",
    "        )\n",
    "\n",
    "    if save_video is not None:\n",
    "        out.release()\n",
    "\n",
    "    return mprec, msucc, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mytc128\"\n",
    "tracker = \"AAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TennisBall_ce', 'Torus', 'Yo-yos_ce1', 'Spiderman_ce', 'Eagle_ce', 'Logo_ce', 'Couple', 'Fish_ce2', 'Basketball', 'Baby_ce', 'Ball_ce3', 'Surf_ce4', 'Skating_ce2', 'Busstation_ce1', 'Cup_ce', 'Deer', 'Yo-yos_ce2', 'Pool_ce2', 'Face_ce2', 'Walking2', 'TableTennis_ce', 'MountainBike', 'Microphone_ce1', 'Fish_ce1', 'Girlmov', 'Pool_ce1', 'Singer1', 'Plate_ce1', 'Sailor_ce', 'Ironman', 'SuperMario_ce', 'Surf_ce2', 'Woman', 'Bolt', 'Bike_ce1', 'Bikeshow_ce', 'Walking', 'Ball_ce1', 'Shaking', 'Ball_ce2', 'Basketball_ce1', 'Basketball_ce2', 'Carchasing_ce1', 'Railwaystation_ce', 'Messi_ce', 'Surf_ce3', 'Ball_ce4', 'Matrix', 'Biker', 'Bee_ce', 'Iceskater', 'Guitar_ce2', 'Skiing', 'Toyplane_ce', 'Plate_ce2', 'Boat_ce1', 'Kite_ce3', 'Surf_ce1', 'Electricalbike_ce', 'Juice', 'FaceOcc1', 'Badminton_ce1', 'Boat_ce2', 'MotorRolling', 'Tennis_ce3', 'Panda', 'Sunshade', 'Hurdle_ce2', 'Singer2', 'Plane_ce2', 'Bicycle', 'Motorbike_ce', 'Airport_ce', 'Liquor', 'Doll', 'Tiger2', 'Boy', 'Hurdle_ce1', 'Skiing_ce', 'Singer_ce1', 'Microphone_ce2', 'Bike_ce2', 'Face_ce', 'Diving', 'Michaeljackson_ce', 'Tiger1', 'Busstation_ce2', 'Lemming', 'Bird', 'CarDark', 'Coke', 'Basketball_ce3', 'Suitcase_ce', 'Kobe_ce', 'Soccer', 'Skating1', 'Ring_ce', 'Badminton_ce2', 'Cup', 'Kite_ce1', 'Carchasing_ce3', 'Kite_ce2', 'Gym', 'Skating2', 'Skating_ce1', 'Yo-yos_ce3', 'Girl', 'Hand', 'Hand_ce2', 'Charger_ce', 'Skyjumping_ce', 'Tennis_ce1', 'Board', 'Tennis_ce2', 'Thunder_ce', 'Hand_ce1', 'Carchasing_ce4', 'Pool_ce3', 'David3', 'Trellis', 'Singer_ce2', 'Crossing', 'Guitar_ce1'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[dataset].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: Tiger1       Time: 30.3s Speed: 11.65fps mSuccess: 0.60 mPrecision 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5020210933957814, 0.5988700564971752, 11.65355821343052)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = \"Tiger1\"\n",
    "t1 = models[tracker]\n",
    "v1 = datasets[dataset][video]\n",
    "\n",
    "video_name = f\"{dataset}_{tracker}_{video}.avi\"\n",
    "track_video(t1, v1, verbose=1, save_video=video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb19bd5b92a045e79a0ddbaeaf1bbdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'RIFF\\xeeC}\\x00AVI LIST\\xec\\x11\\x00\\x00hdrlavih8\\x00\\x00\\x00P\\xc3\\x00\\x00\\x00p\\x17\\x00...', formaâ€¦"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Video, Image\n",
    "\n",
    "new_ = Video.from_file(video_name, play=True, width=360, height=360)\n",
    "new_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_dataset(\n",
    "    tracker, dataset, dataset_name, n=3, verbose=0, save_results=False\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"Track a whole dataset.\n",
    "\n",
    "    Args:\n",
    "        tracker (model.Tracker): Tracker.\n",
    "        dataset (Dict): Dataset.\n",
    "        dataset_name (str): Dataset name.\n",
    "        n (int, optional): Number of videos to track. Defaults to 3.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "        save_results (bool, optional): Save results. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Mean Precisions, Successes, FPSs.\n",
    "    \"\"\"\n",
    "    precisions: List[float] = []\n",
    "    success: List[float] = []\n",
    "    fpss: List[float] = []\n",
    "\n",
    "    result_folder = Path(\"results\")\n",
    "    if save_results:\n",
    "        (result_folder / tracker.model_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results: Dict = {}\n",
    "    count = 0\n",
    "\n",
    "    for video in dataset:\n",
    "        if count == n:\n",
    "            break\n",
    "\n",
    "        precision, succes, fps = track_video(tracker, dataset[video], verbose)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        success.append(succes)\n",
    "        fpss.append(fps)\n",
    "\n",
    "        if save_results:\n",
    "            results[video] = {\"precision\": precision, \"succes\": succes, \"fps\": fps}\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if save_results:\n",
    "        with open(result_folder / tracker.model_name / (dataset_name + \".json\"), 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    return np.mean(precisions), np.mean(success), np.mean(fpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_benchmark(models, datasets, verbose=0, save_results=False):\n",
    "    \"\"\"Benchmark a set of models on a set of datasets.\n",
    "\n",
    "    Args:\n",
    "        models (Dict): Models.\n",
    "        datasets (Dict): Datasets.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "        save_results (bool, optional): Save results. Defaults to False.\n",
    "    \"\"\"\n",
    "    for model_name in models:\n",
    "        for dataset_name in datasets:\n",
    "            print(f\"Benchmarking {model_name} on {dataset_name}\")\n",
    "            track_dataset(\n",
    "                models[model_name],\n",
    "                datasets[dataset_name],\n",
    "                dataset_name,\n",
    "                verbose=verbose,\n",
    "                save_results=save_results,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking SEsiamFC on mytc128\n",
      "Video: TennisBall_ce Time: 7.8s Speed: 36.79fps mSuccess: 0.53 mPrecision 0.38\n",
      "Video: Torus        Time: 10.4s Speed: 25.27fps mSuccess: 0.99 mPrecision 0.77\n",
      "Video: Yo-yos_ce1   Time: 6.7s Speed: 34.78fps mSuccess: 0.74 mPrecision 0.56\n",
      "Benchmarking SEsiamFC on myvot2021\n",
      "Benchmarking AAA on mytc128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leiyks/git/DNN/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: TennisBall_ce Time: 18.2s Speed: 15.74fps mSuccess: 0.55 mPrecision 0.40\n",
      "Video: Torus        Time: 23.3s Speed: 11.27fps mSuccess: 1.00 mPrecision 0.80\n",
      "Video: Yo-yos_ce1   Time: 13.5s Speed: 17.32fps mSuccess: 0.71 mPrecision 0.55\n",
      "Benchmarking AAA on myvot2021\n",
      "Benchmarking PyECO on mytc128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leiyks/git/DNN/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: TennisBall_ce Time: 9.0s Speed: 32.03fps mSuccess: 0.53 mPrecision 0.41\n",
      "Video: Torus        Time: 4.7s Speed: 56.19fps mSuccess: 1.00 mPrecision 0.84\n",
      "Video: Yo-yos_ce1   Time: 4.1s Speed: 56.95fps mSuccess: 0.73 mPrecision 0.56\n",
      "Benchmarking PyECO on myvot2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leiyks/git/DNN/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "do_benchmark(models, datasets, verbose=1, save_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948649c324ca7862321fb27243ea9939e0d68076f558125e86352d86ba9fdaa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
